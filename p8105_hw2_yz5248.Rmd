---
title: "p8105_hw2_yz5248"
author: "yz5248"
date: "2025-09-24"
output: github_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(janitor)
library(readr)
library(readxl)
```

Problem 1.   
First, clean the data in pols-month.csv.  
Breaking up the variable mon into integer variables year, month, and day.  
Replacing month number with month name.  
Creating a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable.
```{r}
pols_df = read_csv("fivethirtyeight_datasets/pols-month.csv")|>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    month = month.name[month],
    president = if_else(prez_gop == 1, "gop", "dem")
  ) |>
  select(-prez_gop, -prez_dem, -day)
pols_df
```


```{r}
snp_df = read_csv("fivethirtyeight_datasets/snp.csv") |>
  separate(date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(if_else(as.integer(year) > 50, paste0("19", year), paste0("20", year))),
    month = as.integer(month),
    month = month.name[month]
  ) |>
  select(year, month, close) |>
  arrange(year, match(month, month.name))
snp_df
```

```{r}
unemp_df = read_csv("fivethirtyeight_datasets/unemployment.csv")|>
  pivot_longer(
    cols = Jan:Dec,
    names_to = "month",
    values_to = "unemployment"
  ) |>
  mutate(month = recode(month,
                        Jan="January", Feb="February", Mar="March",
                        Apr="April", May="May", Jun="June",
                        Jul="July", Aug="August", Sep="September",
                        Oct="October", Nov="November", Dec="December")) |>
  rename(year = Year)
unemp_df

```

```{r}
merged_df = pols_df |>
  left_join(snp_df, by = c("year", "month")) |>
  left_join(unemp_df, by = c("year", "month"))

merged_df
```

```{r}
tibble(
  Rows = nrow(merged_df),
  Columns = ncol(merged_df),
  Year_Min = min(merged_df$year, na.rm = TRUE),
  Year_Max = max(merged_df$year, na.rm = TRUE)
)
```


Problem 2. 

```{r}
mr_trash_wheel = read_excel(
  "202409 Trash Wheel Collection Data.xlsx",sheet = "Mr. Trash Wheel", skip = 1) |>
  janitor::clean_names() |>       # make column names snake_case
  filter(!is.na(dumpster)) |>     # keep only rows with data
  mutate(
    year = as.integer(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr. Trash Wheel"
  )

```

```{r}
prof_trash_wheel = read_excel(
  "202409 Trash Wheel Collection Data.xlsx",
  sheet = "Professor Trash Wheel",
  skip = 1,
) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))|>
  mutate(
    year = as.integer(year)
  )

gwynnda = read_excel(
  "202409 Trash Wheel Collection Data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  skip = 1,
) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))|>
  mutate(
    year = as.integer(year)
  )

trash_wheel_data = bind_rows(mr_trash_wheel, prof_trash_wheel, gwynnda)
```


Problem 3. 
```{r}
zip_df = read_csv(
  "zillow_data/Zip Codes.csv"
  ) |> 
  janitor::clean_names() |>
  mutate(zip = str_pad(as.character(zip_code), 5, pad = "0")) |>
  distinct(zip, .keep_all = TRUE)

zori_wide = read_csv(
  "zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv"
  ) |> 
  janitor::clean_names()
```

```{r}
zori_long = zori_wide |>
  pivot_longer(
    cols = starts_with("x"),  
    names_to = "date",
    values_to = "zori"
  ) |>
  mutate(
    date = ymd(str_remove(date, "^x")),  
    zori = as.numeric(zori),
    zip = str_pad(region_name, 5, pad = "0")  
  ) |>
  drop_na(zori)

view(zori_long)
```


```{r}
zori_merged = zori_long |>
  left_join(zip_df, by = "zip") |>
  select(zip, county, neighborhood, date, zori, everything())

zori_merged
```

The dataset contains `r nrow(zori_merged)` total observations, covering `r n_distinct(zori_long$zip)` unique ZIP codes in the ZORI data and `r n_distinct(zip_df$zip)` unique ZIP codes in the lookup table. The number of unique neighborhoods is `r n_distinct(zip_df$neighborhood)`.






